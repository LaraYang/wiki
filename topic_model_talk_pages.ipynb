{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T22:18:17.279255Z",
     "start_time": "2022-03-13T22:18:07.127412Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "from convokit import Corpus\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import time\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import contractions\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import multiprocessing\n",
    "num_cores = 10\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "nlp.max_length = 1800000\n",
    "home_dir = \"/Volumes/Extrema/wiki/conv_data\"\n",
    "model_dir = \"/Users/Lara/Documents/Stanford/Research/Network/processed_data/lda_models\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T22:21:53.827917Z",
     "start_time": "2022-03-13T22:18:17.280863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mWARNING: \u001b[0mCorpusLoadWarning: Missing speaker metadata for speaker ID: None. Initializing default empty metadata instead.\n"
     ]
    }
   ],
   "source": [
    "wiki_corpus = Corpus(filename=os.path.join(home_dir, \"wikiconv-2018\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Text for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T22:21:53.832873Z",
     "start_time": "2022-03-13T22:21:53.829837Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "stop_words = STOP_WORDS\n",
    "custom_stop_words = ['wiki', 'wikipedia', 'hi', 'page', 'talk', 'article']\n",
    "stop_words = set(list(stop_words) + custom_stop_words)\n",
    "# getting rid of these tokens that are not included in the pre-defined punctuation list\n",
    "# no better lists existing\n",
    "punctuation = string.punctuation + '–...…’“”•'\n",
    "# removing bullet points and numbers\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T22:21:54.204188Z",
     "start_time": "2022-03-13T22:21:53.842214Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "bots_df = pd.read_csv('/Users/Lara/Documents/Stanford/Research/Network/processed_data/all_bots.csv')\n",
    "bots = bots_df['0'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA expects documents to be in a list format, where each element is a document. Given the short nature of comments, the reasonable document segmentation can either be a page or a single conversation. The former will tell us roughly what kind of conversations are going on in talk pages, while the latter tells us what are the minute details of each conversation. So let's go with the former first. We'll create two LDA model, for both user talk and talk pages, for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T22:21:53.839950Z",
     "start_time": "2022-03-13T22:21:53.834472Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text, package):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        Text to be tokenized and lemmatized\n",
    "    package : str\n",
    "        One of \"nltk\" or \"spacy\". Specifies which pakcage to use for lemmatization.\n",
    "        NTLK is faster, spaCy is more accurate and takes POS into account.\n",
    "    Returns\n",
    "    -------\n",
    "    cleaned_toks : list of str\n",
    "        A list of cleaned tokens\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = contractions.fix(text)\n",
    "    except IndexError as e:\n",
    "        print(\"%s led to an IndexError\" % text)\n",
    "        print(e)\n",
    "    text = text.lower().strip()\n",
    "    if package == 'spacy':\n",
    "        cleaned_toks = [t.lemma_ for t in nlp(text) if re.match('^[a-z]+$', str(t)) and str(t) not in stop_words]\n",
    "    elif package == 'nltk':\n",
    "        cleaned_toks = []\n",
    "        try:\n",
    "            cleaned_toks = [lemmatizer.lemmatize(t) for t in tokenizer.tokenize(text)\n",
    "                        if re.match('^[a-z]+$', t) and t not in stop_words]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    return cleaned_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T22:29:50.108305Z",
     "start_time": "2022-03-13T22:29:50.032208Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_by_id(id_type, remove_bots):\n",
    "    \"\"\"\n",
    "    Clean and separate all utterances into documents for topic modeling\n",
    "    Parameters\n",
    "    ----------\n",
    "    id_type : str\n",
    "        The type of identifier to be used for document separation, one of\n",
    "        page_id or conversation_id\n",
    "    remove_bots : bool\n",
    "        If true, remove comments made by bots\n",
    "    Returns\n",
    "    -------\n",
    "    id2texts : dict of {str : list of str}\n",
    "        A dictionary mapping document IDs, whose type is specified by id_type,\n",
    "        to a list of tokens\n",
    "    \"\"\"\n",
    "    l = list(wiki_corpus.iter_utterances())\n",
    "    id2texts=defaultdict(list)\n",
    "    for utt in tqdm(l):\n",
    "        if remove_bots:\n",
    "            userid = utt.speaker.id\n",
    "            if userid in bots:\n",
    "                continue\n",
    "        text = clean_text(utt.text, 'spacy')\n",
    "        if id_type == 'page_id':\n",
    "            doc_id, text = utt.get_conversation().meta[id_type], text\n",
    "        elif id_type == 'conversation_id':\n",
    "            doc_id, text = utt.get_conversation().id, text\n",
    "        else:\n",
    "            raise NotImplementedError(\"Document ID type not recognized\")\n",
    "        if len(text) > 0:\n",
    "            id2texts[doc_id].extend(text)\n",
    "    return id2texts\n",
    "\n",
    "def separate_by_type(id2texts, id_type):\n",
    "    \"\"\"\n",
    "    Separate documents by the type of talk page to which they belong\n",
    "    Parameters\n",
    "    ----------\n",
    "    id2texts : dict of {str : list of str}\n",
    "        A dictionary mapping document IDs to documents represented by a list of tokens\n",
    "    id_type : str\n",
    "        The type of identifier that is used for document separation, one of \n",
    "        page_id or conversation_id\n",
    "    Returns\n",
    "    -------\n",
    "    (usertalks, talks, projtalks) : Tuple of (list of list of str, list of list of str, list of list of str)\n",
    "        Returns a tuple of three lists that contains usertalk documents, talk\n",
    "        documents, and project talk documents in that sequence\n",
    "    \"\"\"\n",
    "    id2article_type = {}\n",
    "    if id_type == 'page_id':\n",
    "        for conv in tqdm(wiki_corpus.iter_conversations()):\n",
    "            if conv.meta[id_type] not in id2article_type:\n",
    "                id2article_type[conv.meta[id_type]] = conv.meta['page_type']\n",
    "    elif id_type == 'conversation_id':\n",
    "        for conv in tqdm(wiki_corpus.iter_conversations()):\n",
    "            if conv.id not in id2article_type:\n",
    "                id2article_type[conv.id] = conv.meta['page_type']\n",
    "    else:\n",
    "        raise NotImplementedError(\"Document ID type not recognized\")\n",
    "    usertalks, talks, projtalks = [], [], []\n",
    "    for doc_id, texts in id2texts.items():\n",
    "        page_type = id2article_type[doc_id]\n",
    "        if page_type == 'user_talk':\n",
    "            usertalks.append(texts)\n",
    "        elif page_type == 'talk':\n",
    "            talks.append(texts)\n",
    "        elif page_type == 'wikipedia_talk':\n",
    "            projtalks.append(texts)\n",
    "    \n",
    "    return (usertalks, talks, projtalks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing by page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T22:32:12.570831Z",
     "start_time": "2022-03-13T22:30:03.170863Z"
    }
   },
   "outputs": [],
   "source": [
    "pageid2texts = parse_by_id('page_id', remove_bots=True)\n",
    "usertalks, talks, projtalks = separate_by_type(pageid2texts, 'page_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T22:32:12.572669Z",
     "start_time": "2022-03-13T22:32:04.124Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(model_dir, 'cleaned_talks_human.txt'), 'w') as f:\n",
    "    for doc in talks:\n",
    "        f.write(' '.join(doc) + '\\n')\n",
    "\n",
    "with open(os.path.join(model_dir, 'cleaned_user_talks_human.txt'), 'w') as f:\n",
    "    for doc in usertalks:\n",
    "        f.write(' '.join(doc) + '\\n')\n",
    "\n",
    "with open(os.path.join(model_dir, 'cleaned_proj_talks_human.txt'), 'w') as f:\n",
    "    for doc in projtalks:\n",
    "        f.write(' '.join(doc) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing by conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T00:54:44.430050Z",
     "start_time": "2022-03-13T22:32:56.021705Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f464ddbda9a404a8414a1c291ebf4eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5002011.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "İlk WP maddemi açtım: Kurtuluş belgesel filmi! Lütfen biraz yardım et bana. Öncelikle maddeyi arayanların kolay bulması için birşeyler yapalım (Kurtuluş-Kurtulus vb). Çok ilginçtir, gemiyi ararken bile \"Kurtuluş gemisi\" yazınca çıkmıyor, SS yazmak ve \"ş\" kullanmak gerekiyor! ''Yani o madde de yardım bekliyor''. Hadi beni kutlamak yerine '''develop this article'''. Thanks in advance.  \n",
      "Hayırlı olsun ) Aynı isimde başka madde yoksa parantez kullanımı gereksiz madde adlarında. Dolayısıyla parantezli kısmın kaldırılması gerek madde adından. Yine film, İngilizce gösterim adıyla adlandırılmalı. Bu film İngilizce olarak vizyona girdi mi? IMDb'de farklı bir adlandırılma yapılmış mesela. Aksi bir durum yoksa filmin orijinal adı olan Türkçe adı tercih edilmeli. Gemi maddesine gelirsek, sadece \"Kurtuluş\" adı alabilecek iki bir madde var burada: İstanbul'daki Kurtuluş semti ve Silifke'nin köyü olan Kurtuluş. Aklımıza ilk semt geldiğinden sade ad olan \"Kurtuluş\" adı, semti anlatan maddeye verilmiş, üst kısmına da \"Başka Kurtuluşlar da var, onlara da bakın\" gibisinden şuradaki anlam ayrımına bağlantı verilmiş. Geminin adı ise sadece Kurtuluş değil, \"SS Kurtuluş\". O yüzden Kurtuluş yazınca direk geminin çıkması doğru olmuyor, olamıyor )  led to an IndexError\n",
      "string index out of range\n",
      "@, the reason why İYİ is attracting deputies and voters primarily from CHP and MHP is that these parties are all rooted in different types of Turkish nationalism. CHP is left-wing nationalist, while MHP is right-wing nationalist, and they both idolize Atatürk as a national figure despite somewhat diverging views on his later domestic reforms. This admiration is something they also share with İYİ. Relatively speaking, and by Turkish standards, İYİ could be considered a centrist nationalist party, positioned between CHP and MHP. Due to a shared 'atatürkist' foundation, İYİ gets along well with CHP. Conversely, İYİ's natural rivals are HDP (since that party is Kurdish nationalist) and the AKP (since that party is pro-RTE and Islamic-rooted. To answer your question, the relationship with the HDP is non-existent; İYİ stated they would not join Millet if HDP was to become a member as well. And I'd be surprised to see AKP figures switching to İYİ, considering the latter was formed explicitly to take on the former. The left-right divide is not as trong in Turkey as in many Western countries; there is a stronger divide over ethnic and religious cleavages.  led to an IndexError\n",
      "string index out of range\n",
      " Thanks for the response on this . I was the user who renamed it to \"İyi Party\", and am aware of the intention behind spelling it using capitals. That said, we have policies surrounding the naming of articles (the relevant ones are MOS:TMRULES and WP:TITLEFORMAT). On a case-by-case basis, we follow the conventions that are used by independent, reliable sources. In this case, nearly all of the third party sources that I came across (e.g. newspapers, websites with editorial oversight, etc.) spell it \"İyi\" rather than \"İYİ\", despite the fact that it seems to miss the point (and note that I kept \"İYİ\" in the lead following MOS:TM – this part of the lead could certainly be expanded to explain the connection to the flag if you have a reliable source, either as a very brief inline note, or more expansively as a footnote). That said, it might be helpful to get some third opinions, maybe at the Village Pump or WT:MOSTM (for the record, I sort of want you to be right, as anything that might help opposition parties in Turkey right now seems like a good thing! Perhaps check some of the other examples of Trademarks from corporations that don't follow sentence case if you want to make a strong argument that \"İYİ\" conforms to policy? But ultimately, Wikipedia as a whole has to be neutral, and apply policies in an evenhanded way, so unless there's a good argument based on policy it's unlikely to stick. Good luck!) ‑‑ led to an IndexError\n",
      "string index out of range\n",
      "Hi. If you want to go full WP:LAWYER, let's go full WP:LAWYER. I definitely agree on your point about needing consensus. Unfortunately for you, consensus already exists in that two editors in this discussion support the use of the name İYİ as opposed to yourself, the only editor who proposes otherwise. Crucially, the two editors in favour of the change have compelling arguments for doing so as discussed above (see WP:CONLEVEL), as opposed to your counter-arguments, which I would argue represent a misunderstanding of Wikipedia naming guidelines (explained below). Thus, your attempt to involve external editors despite a consensus already being established is an act of WP:GAME (and WP:FILIBUSTER in particular). In short, your revert is a violation of WP:CONSENSUS, your justification attempt an act of WP:LAWYERING, and hence, your actions as a whole are WP:DISRUPTIVE. It appears you have missed the most important Wikipedia:Official names policy in this case, namely WP:NCPARTY, which is the relevant naming guideline for a political party such as İYİ. The guideline states, and I quote:\n",
      "''The title used in reliable English-language sources both inside and outside the political party's country (in scholarly works and in the news media), should be preferred''\n",
      "Additionally, MOS:TM states: ''Follow standard English text formatting and capitalization practices, even if the trademark owner considers nonstandard formatting \"official\", '''as long as this is a style already in widespread use''', rather than inventing a new one''\n",
      "Both of these come back to the issue of reliable sources, of which I alleged that twice as many exist for İYİ than İyi. Of course I'm not going to list every single one for your satisfaction, but what we can do is do a general google search for the number of English sources that exist for each. Searching for English news articles entitled 'İYİ Party' yields about 131,000. Since google searches cannot be case sensitive, no exact result for a search for 'İyi Party' exists, so I can't prove that twice as many sources exist for İYİ rather than İyi. But, if you search 'İyi Party' and take a look at the first 3 results pages, only 3 of the 30 results actually use 'İyi Party'. The rest all use the capitalised version İYİ. Furthermore, the three sources using 'İyi' spell it wrong, so are hardly 'credible sources'. So I would like to revise my original claim that I could find twice as many sources that use 'İYİ' rather than 'İyi'. I can in fact find '''10 times more''' sources that use 'İYİ' rather than 'İyi'. Hence, we can conclude:\n",
      "The title used in reliable English-language sources both inside and outside the political party's country (in scholarly works and in the news media), is '''İYİ''' and not 'İyi'. As per WP:NCPARTY, '''İYİ''' is therefore preferred.\n",
      "The google search of sources reveal, at a 90% confidence level, that 'İyi' is not ''a style already in widespread use'', and hence does not satisfy the criteria within MOS:TM.\n",
      "Thus, by the powers vested in us editors under WP:CONSENSUS and in accordance to the guidelines WP:NCPARTY and MOS:TM, I am hereby seeking administrator approval to revert your edit back to '''İYİ Party''' and rest my case. I hope I don't have to respond to any more acts of WP:LAWYERING, WP:GAME, WP:FILIBUSTER or WP:DISRUPTIVE from you again. It has been a pleasure wasting hours on this with you.   led to an IndexError\n",
      "string index out of range\n",
      "REDIRECT Talk:Fatih Arda İpcioğlu led to an IndexError\n",
      "string index out of range\n",
      "Well, believe it or not, some western media actually does use simply 'AK' to refer to the AK Parti, while in Turkey, it is extremely common to use simply 'Saadet' for Saadet Partisi. Meanwhile, the use of İP is close to non-existent. Yes, it is occasionally used, but not even remotely close to as much as İYİ, not by a long-shot. Considering there is no doubt that the full 'İYİ' designation is (1) more commonly used, and (2) the party's own preference, not to mention it is more reflective of the party's identity, in that it is meant to symbolize the Kayı banner. That banner is essentially the party's ''only'' reason for picking the name and color. If parties choose to embrace a specific, deliberate branding in the way İYİ does, then we have no right to impose our own obscure, \"artificial\" abbreviation on it. The only statement that comes close to being an argument for a change, namely \"this is how some other countries do it\", does not count for anything here. Finally, let me just ask you to go on Google or, better yet, Google Images, and search for the word \"iyi\". That should make it completely clear to anyone in doubt that İYİ is the abbreviation of this party. Meanwhile, search results related to \"İP\" in the context of Turkish politics will yield results for the now-rebranded Workers' Party (İşçi Parti), which could cause confusion.  led to an IndexError\n",
      "string index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Draft:HİNDARX ŞƏHİDLƏRİ, a page which you created or substantially contributed to, has been nominated for deletion. Your opinions on the matter are welcome; you may participate in the discussion by adding your comments at Wikipedia:Miscellany for deletion/Draft:HİNDARX ŞƏHİDLƏRİ and please be sure to sign your comments with four tildes (~~~~). You are free to edit the content of Draft:HİNDARX ŞƏHİDLƏRİ during the discussion but should not remove the miscellany for deletion template from the top of the page; such a removal will not end the deletion discussion. Thank you.   led to an IndexError\n",
      "string index out of range\n",
      "Evlidir,üç qızı bir  oğlu, altı  nəvəsi var(İlham,Kənan,Murad,Mələk,Ziya,Elmir).\n",
      "Xanımı Məmmədova Rəna İbrahim qızı orta məktəbdə müəllimədir.Böyük qızı Məmmədova (Mayılova) Aygün ixtisasca həkimdir ,alimlik dərəcəsi müdafiə etmişdir tibb üzrə fəlsəfə doktorudur;2-ci qızı Məmmədova Ülkər ixtisasca stomatoloqdur,tibb kollecində müəllimə kimi calışır;3-cü qızı Məmmədova (Abdullayeva) Ülviyyə Ü.Hacıbəyov adına Musiqi Akademiyasını fərqlənmə diplomu ilə bitirib musiqi məktəbində fortepiano müəlliməsidir;oğlu Məmmədli Əli (1997) hal hazirda İtaliyada Romanın ən qabaqcil Universitetlərindən birində ali təhsil alır .\n",
      "Atası Məmmədov Əli Məmmədsalah oğlu(1908-1994) rayonun sayılıb secilen ağsaqqallarından oiub,müxtəlif vəzifələrdə calışıb\n",
      "Anası Məmmədova Ceyran Nəcəf qızı 1912–ci ildə   anadan olub, 1990–cı ildə vəfat edib.\n",
      "Professor Əliniyaz Məmmədovun həyatda uğur qazanmasında  böyük qardaşı Əlihüseyn müəllimin böyük rolu olmuşdur led to an IndexError\n",
      "string index out of range\n",
      "Əliniyaz Məmmədov 1987-cü ildə Bakı şəhərində ildə N.Nərimanov adına Azərbaycan Dövlət Tibb İnstitutunda  '''aspiranturaya''' daxil olmuşdur. 1989-ci ildə aspirantura müddəti başa çatmamış \"Эндолимфатическая комплексная медикаментозная терапия в коррекции нарушений микроциркуляции  и детоксикации организма при распространенном перитоните\" mövzusunda namizədlik dissertasiyası müdafiə edərək vaxtından əvvəl tibb elmləri namizədi alimlik dərəcəsinə layiq görülmüşdür.\n",
      "Əliniyaz Məmmədov  '''aspiranturanı''' bitirdikdən sonraN.Nərimanov adına '''Azərbaycan Dövlət Tibb İnstitutunun'''  cərrahi xəstəliklər kafedrasında  assistent  sonra isə  həmin kafedrada dosent vəzifəsində işləmişdir.\n",
      "2005-ci  ildə Ali Attestasiya Komissiyasının göndərişi ilə  Moskva şəhərində \"Использование магнито-ИК лазерного излучения в профилактике и комплексном лечении печеночной недостаточности при механической  желтухе неопухолевого генеза \" mövzusunda doktorluq dissertasiyası müdafiə etmişdir.\n",
      "2007-ci ildə Azərbaycan Respublikası Prezidenti yanında Ali Attestasiya komissiyası tərəfindən diplomu qəbul edilərək ona, Azərbaycan Respublikasının tibb elmləri doktoru elmi dərəcəsi verilmişdir\n",
      "2007-cı ildən Odlar Yurdu Universitetində cərrahi xəstəliklər kafedrasının yaranmasında və təçkilatlanmasında bilavasitə iştirak edərək uzun müddət kafedra müdiri vəzifəsidə calışmışdır.\n",
      "2013-cü ildən Ə.Əliyev adına Azərbaycan Dövlət Həkimləri Təkmilləşdirmə İnstitutunun cərrahi xəstəliklər kafedrasının professorudur. led to an IndexError\n",
      "string index out of range\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6bcdd063f314058bb2888375a8e5476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pageid2texts = parse_by_id('conversation_id', remove_bots = True)\n",
    "usertalks, talks, projtalks = separate_by_type(pageid2texts, 'conversation_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T00:54:47.313123Z",
     "start_time": "2022-03-14T00:54:44.432864Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dir = \"/Users/Lara/Documents/Stanford/Research/Network/processed_data/lda_models\"\n",
    "with open(os.path.join(model_dir, 'talks_by_conv_human.txt'), 'w') as f:\n",
    "    for doc in talks:\n",
    "        f.write(' '.join(doc) + '\\n')\n",
    "\n",
    "with open(os.path.join(model_dir, 'user_talks_by_conv_human.txt'), 'w') as f:\n",
    "    for doc in usertalks:\n",
    "        f.write(' '.join(doc) + '\\n')\n",
    "\n",
    "with open(os.path.join(model_dir, 'proj_talks_by_conv_human.txt'), 'w') as f:\n",
    "    for doc in projtalks:\n",
    "        f.write(' '.join(doc) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient Lemmatizing Using Batch SpaCy NLP Pipeline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-25T00:48:55.674Z"
    }
   },
   "source": [
    "l = list(wiki_corpus.iter_utterances())\n",
    "# pageid to list of strings\n",
    "pageid2texts=defaultdict(list)\n",
    "for utt in tqdm(l):\n",
    "    pid, text = utt.get_conversation().meta['page_id'], utt.text\n",
    "    if len(text.strip()) > 0:\n",
    "        pageid2texts[pid].append(text)\n",
    "\n",
    "#pageid to list of toks\n",
    "pageid2clean_texts = defaultdict(list)\n",
    "for pageid, texts in tqdm(pageid2texts.items()):\n",
    "    clean_text = []\n",
    "    for text in nlp.pipe(texts, n_process=8, disable=[\"parser\", \"ner\"]):\n",
    "        cleaned_toks = [t.lemma_ for t in text if re.match('^[a-z]+$', str(t)) and str(t) not in stop_words]\n",
    "        clean_text.extend(cleaned_toks)\n",
    "    pageid2clean_texts[pageid] = clean_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Topic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-27T05:59:13.669632Z",
     "start_time": "2021-11-27T05:59:00.779336Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_texts(model_dir, talk_fn, user_talk_fn, proj_talk_fn):\n",
    "    \"\"\"\n",
    "    Loads cleaned texts from disk to save time from repeat processing.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_dir : str\n",
    "        Location of model\n",
    "    talk_fn : str\n",
    "        Filepath to cleaned talk pages\n",
    "    user_talk_fn : str\n",
    "        Filepath to cleaned user talk pages\n",
    "    proj_talk_fn : str\n",
    "        Filepath to cleaned project talk pages\n",
    "    Returns\n",
    "    -------\n",
    "    (talks, usertalks, projtalks) : Tuple of lists\n",
    "        A tuple of cleaned documents of three talk page types\n",
    "    \"\"\"\n",
    "    talks, usertalks, projtalks = [], [], []\n",
    "    with open(os.path.join(model_dir, talk_fn)) as f:\n",
    "        for line in f:\n",
    "            talks.append(line.split())\n",
    "\n",
    "    with open(os.path.join(model_dir, user_talk_fn)) as f:\n",
    "        for line in f:\n",
    "            usertalks.append(line.split())\n",
    "\n",
    "    with open(os.path.join(model_dir, proj_talk_fn)) as f:\n",
    "        for line in f:\n",
    "            projtalks.append(line.split())\n",
    "    return (talks, usertalks, projtalks)\n",
    "\n",
    "talks, usertalks, projtalks = load_texts(model_dir, \"talks_by_conv_human.txt\", \"user_talks_by_conv_human.txt\", \"proj_talks_by_conv_human.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T00:56:32.864708Z",
     "start_time": "2022-03-14T00:56:32.856856Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_lda(docs, num_topics, passes, fn, model_dir):\n",
    "    \"\"\"\n",
    "    Workhorse function for generating LDA models.\n",
    "    Parameters\n",
    "    ----------\n",
    "    docs : list of str\n",
    "        A list of documents, where each item is a document\n",
    "    num_topics : int\n",
    "        The number of topics to train\n",
    "    passes : int\n",
    "        The number of times to iterate over the corpus\n",
    "    fn : str\n",
    "        The file prefix used for saving all corresponding files produced\n",
    "        in the generation of topic models\n",
    "    model_dir : str\n",
    "        Path to output directory for LDA models and other auxilary files\n",
    "    \"\"\"\n",
    "    dictionary = corpora.Dictionary(docs)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    pickle.dump(corpus, open(os.path.join(model_dir, fn+'_corpus.pkl'), 'wb'))\n",
    "    dictionary.save(os.path.join(model_dir, fn+'_dictionary.gensim'))\n",
    "    model = gensim.models.LdaMulticore(corpus, num_topics = num_topics, id2word=dictionary, passes=passes, workers=9)\n",
    "    model.save(os.path.join(model_dir, fn+'_model.gensim'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T00:56:37.062877Z",
     "start_time": "2022-03-14T00:56:33.424110Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "import jupyternotify\n",
    "ip = get_ipython()\n",
    "ip.register_magics(jupyternotify.JupyterNotifyMagics)\n",
    "\n",
    "NUM_TOPICS = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T01:36:14.178981Z",
     "start_time": "2022-03-14T01:04:15.257469Z"
    }
   },
   "outputs": [],
   "source": [
    "%%notify\n",
    "generate_lda(docs=usertalks, num_topics=NUM_TOPICS, passes=10, fn='usertalk_by_conv_human', model_dir=model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T01:04:15.253296Z",
     "start_time": "2022-03-14T00:57:28.750649Z"
    }
   },
   "outputs": [],
   "source": [
    "generate_lda(docs=talks, num_topics=NUM_TOPICS, passes=10, fn='talk_by_conv_human', model_dir=model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T01:52:53.992499Z",
     "start_time": "2022-03-14T01:52:42.445848Z"
    }
   },
   "outputs": [],
   "source": [
    "generate_lda(docs=projtalks, num_topics=5, passes=5, fn='projtalk_by_conv_human', model_dir=model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
