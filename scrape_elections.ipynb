{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_name(name):\n",
    "    \"\"\"\n",
    "    Parses a string into a user name in the user name and alternative name, contained in parentheses\n",
    "    \"\"\"\n",
    "    m = re.search(r'\\(.+\\)', name)\n",
    "    user_name = re.sub(r'\\(.+\\)', '', name).strip()\n",
    "    alternative_name = ''\n",
    "    if m:\n",
    "        text = m.group(0)\n",
    "        if 'nomination' not in text and '2nd' not in text and '3rd' not in text and '4th' not in text:\n",
    "            alternative_name = re.sub(\"\\(|\\)\", \"\", text).strip()\n",
    "    return (user_name, alternative_name)\n",
    "\n",
    "def parse_votes_from_items(nested_items):\n",
    "    \"\"\"\n",
    "    Identify the header of a support, oppose, or neutral list from nested items, and calculate \n",
    "    the number of votes based on the lengths of the lists\n",
    "    \"\"\"\n",
    "    support_list, oppose_list, neutral_list = None, None, None\n",
    "    for ni in nested_items:\n",
    "        text = ni.text.strip()\n",
    "        if text == 'Support' or text == 'Support:':\n",
    "            support_list = ni.findNext('ol')\n",
    "        elif text == 'Oppose' or text == 'Oppose:':\n",
    "            oppose_list = ni.findNext('ol')\n",
    "        elif text == 'Neutral' or text == 'Neutral:' or text == 'Abstain' or text == 'Abstain:':\n",
    "            neutral_list = ni.findNext('ol')\n",
    "    for l in [support_list, oppose_list, neutral_list]:\n",
    "        # color changes in some lists lead part of the list to be children of the original list\n",
    "        if l:\n",
    "            for match in l.findAll('font'):\n",
    "                match.replaceWithChildren()\n",
    "    #exclude nested lists; each item is a single vote                        \n",
    "    yes = len(support_list.find_all('li', recursive=False)) if support_list else 0\n",
    "    no = len(oppose_list.find_all('li', recursive=False)) if oppose_list else 0\n",
    "    neutral = len(neutral_list.find_all('li', recursive=False)) if neutral_list else 0\n",
    "    return (yes, no, neutral)\n",
    "\n",
    "def find_votes(nested_soup):\n",
    "    \"\"\"\n",
    "    Identify votes from archived election discussion pagei. Most pages seem to have already tallied\n",
    "    up the votes in the format of (#yes/#no/#neutral) in the pages, which this function tries to identify first.\n",
    "    If unavailable, the function looks for comments and tallies up comments as votes.\n",
    "    \"\"\"\n",
    "    text = nested_soup.find(text=re.compile(r'.\\(\\d+/\\d+/\\d+\\).'))\n",
    "    if text:\n",
    "        m = re.search(r'.\\((\\d+)/(\\d+)/(\\d+)\\).', text)\n",
    "        yes, no, neutral = m.groups()\n",
    "    else:\n",
    "        yes, no, neutral = 0, 0, 0\n",
    "        for nested_items in [nested_soup.find_all('p'), nested_soup.find_all(\"span\", {\"class\":'mw-headline'}), \n",
    "                            nested_soup.find_all(\"dt\")]:\n",
    "            yes, no, neutral = parse_votes_from_items(nested_items)\n",
    "            if not (yes == 0 and no == 0 and neutral == 0):\n",
    "                break\n",
    "    return (yes, no, neutral)\n",
    "\n",
    "def parse_date(time_str):\n",
    "    \"\"\"\n",
    "    Used to convert dates into datetime objects\n",
    "    \"\"\"\n",
    "    parsed = None\n",
    "    time_str = time_str.replace('Novmber', 'November')\n",
    "    for fmt in ['%d %B %Y', '%d %B %y', '%d%B%Y', '%d %B%Y', '%d %b %Y', '%B %Y', ]:\n",
    "        try:\n",
    "            parsed = datetime.strptime(time_str, fmt)\n",
    "        except:\n",
    "            continue\n",
    "    return parsed\n",
    "\n",
    "def parse_time(time_str):\n",
    "    \"\"\"\n",
    "    Used to convert dates with actual timestamps into datetime objects\n",
    "    \"\"\"\n",
    "\n",
    "    parsed = None\n",
    "    time_str = re.sub('\\(UTC\\)', '', time_str)\n",
    "    for fmt in ['%H:%M, %d %b %Y', '%H:%M, %b %d, %Y', '%H:%M, %Y %b %d', '%H:%M, %d %B %Y', '%H:%M, %B %d, %Y', '%H:%M, %Y %B %d']:\n",
    "        try:\n",
    "            parsed = datetime.strptime(time_str.strip(), fmt)\n",
    "        except:\n",
    "            continue\n",
    "    return parsed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_re = [r'\\d+:\\d+, \\d+ [A-Za-z]+ \\d+ \\(UTC\\)', r'\\d+:\\d+, [A-Za-z]+ \\d+,? \\d+ \\(UTC\\)']\n",
    "def is_self_nominate(content, link, user_name):\n",
    "    \"\"\"\n",
    "    Determines whether an individual is self-nominated by comparing username to signature and finding key words\n",
    "    in text\n",
    "    \"\"\"\n",
    "    # assuming user name is the first h3 title\n",
    "    title = content.find('h3')\n",
    "    nomination = ''\n",
    "    self_nominate = False\n",
    "    if title.span.text == user_name or re.sub(' ', '_', title.span.text) == user_name:\n",
    "        for tag in title.next_siblings:\n",
    "            # newlines\n",
    "            if isinstance(tag, bs4.element.NavigableString):\n",
    "                continue\n",
    "            if tag.name == 'ul':\n",
    "                for i in tag.find_all('li'):\n",
    "                    i.replaceWithChildren()\n",
    "                for i in tag.find_all('ul'):\n",
    "                    i.decompose()\n",
    "            if 'Questions for the candidate' in tag.text or 'Support' in tag.text:\n",
    "                break\n",
    "            if tag.find('a'):\n",
    "                for c in tag.find_all('a'):\n",
    "                    # matches on cases where the user no longer exists and thus href link does not match to a specific user\n",
    "                    if (c.has_attr('title') and 'User:' in c['title']):\n",
    "                        name = c['title'][c['title'].find(':')+1:]\n",
    "                        name = re.sub('\\(page does not exist\\)', '', name)\n",
    "                        # user_names are derived from the link which cannot contain spaces,\n",
    "                        # titles can contain spaces, and thus we need to replace titles with underscores\n",
    "                        # which are used in links\n",
    "                        c.string = re.sub(' ', '_', name)\n",
    "                    elif (c.has_attr('title') and 'User talk:' in c['title']):\n",
    "                        name = c['title'][c['title'].find(':')+1:]\n",
    "                        name = re.sub('\\(page does not exist\\)', '', name)\n",
    "                        c.string = re.sub(' ', '_', name)\n",
    "            text = tag.text\n",
    "            if text:\n",
    "                nomination += text\n",
    "    # the current method still doesn't work because the tag of the href is added post time-stamp. Can we replace the href text with its title when iterating?\n",
    "    chop = nomination.find('Candidate, please indicate acceptance')\n",
    "    if chop >= 0:\n",
    "        nomination = nomination[:chop]\n",
    "    for date in date_re:\n",
    "        # allowing for 12 random characters between user name and date to deem \n",
    "        m = re.search(user_name + '[^\\n]{1,12}' + date + \"$\", nomination)\n",
    "        if m:\n",
    "            self_nominate = True\n",
    "            break\n",
    "    if not self_nominate:\n",
    "        self_nom_indicators = ['self-nominat', 'self nominat', 'nominate myself', 'present myself', 'submit myself', 'self-nom']\n",
    "        nomination = nomination.lower()\n",
    "        for i in self_nom_indicators:\n",
    "            if nomination.find(i) > -1:\n",
    "                self_nominate = True\n",
    "                break\n",
    "    return self_nominate\n",
    "\n",
    "def find_comments(l):\n",
    "    \"\"\"\n",
    "    Given a list object, iterate through all items to extract comments, their associated dates, and the user\n",
    "    who left the comment\n",
    "    \"\"\"\n",
    "    comments = []\n",
    "    for i in l.find_all('li', recursive=False):\n",
    "        if i.contents is None or len(i.contents) == 0:\n",
    "            continue\n",
    "        comment = ''   \n",
    "        date = ''\n",
    "        name = ''\n",
    "        for l in [i.find_all('b'), i.find_all('tt'), i.find_all('font'), i.find_all('strong')]:\n",
    "            for match in l:\n",
    "                match.replaceWithChildren()\n",
    "        for element in i:\n",
    "            if isinstance(element, bs4.element.NavigableString):\n",
    "                for r in date_re:\n",
    "                    m = re.search(r, element)\n",
    "                    if m:\n",
    "                        date = m.group()\n",
    "                        element = re.sub(r, '', element)\n",
    "                comment += element\n",
    "            elif element.name == 'a':\n",
    "                # matches on cases where the user no longer exists and thus href link does not match to a specific user\n",
    "                if name == '' and (element.has_attr('title') and 'User:' in element['title']):\n",
    "                    name = element['title'][element['title'].find(':')+1:]\n",
    "                    name = re.sub('\\(page does not exist\\)', '', name)\n",
    "                elif name == '' and (element.has_attr('title') and 'User talk:' in element['title']):\n",
    "                    name = element['title'][element['title'].find(':')+1:]\n",
    "                    name = re.sub('\\(page does not exist\\)', '', name)\n",
    "                elif not ((element.has_attr('title') and 'User talk:' in element['title'])\n",
    "                          or (element.has_attr('title') and 'User:' in element['title'])):\n",
    "                    comment += element.text\n",
    "        comments.append([date, name, comment.strip()])\n",
    "    return comments\n",
    "\n",
    "def extract_comments(content, link):\n",
    "    \"\"\"\n",
    "    Finds vote lists in content and extracts each comment line by line by calling find_comments\n",
    "    \"\"\"\n",
    "    all_comments = []\n",
    "    user_name = link.split('/')[-1]\n",
    "    user_name = re.sub(r'_[1-9]', '', user_name)\n",
    "    for list_name in ['Support', 'Oppose', 'Neutral']:\n",
    "        l = content.find('span', {'id':list_name})\n",
    "        if not l:\n",
    "            for elem in ['dl', 'dt', 'p', 'b']:\n",
    "                l = content.find(elem, text=re.compile('\\s*' + list_name + ':?\\s*'))\n",
    "                if l: break\n",
    "        \n",
    "        if l:\n",
    "            found_list = l.find_next('ol')\n",
    "            if found_list:\n",
    "                comments = find_comments(found_list)\n",
    "                for c in comments:\n",
    "                    all_comments.append([list_name] + c)\n",
    "#    data from 2003 and 2004 are malformed\n",
    "    if len(all_comments) == 0:\n",
    "        comments = []\n",
    "        if content.find('div', {'id':'toc'}): content.find('div', {'id':'toc'}).decompose()\n",
    "        for tag in content.find('div', {'class':'mw-parser-output'}).find_all('ul', recursive=False):\n",
    "            # could either be at the comment level or the nomination level\n",
    "            for i in tag.find_all('li', recursive=False):\n",
    "                nested = i.find_all('ul', recursive=False)\n",
    "                if nested:\n",
    "                    for l in nested:\n",
    "                        comments.extend(find_comments(l))\n",
    "            comments.extend(find_comments(tag))\n",
    "        for c in comments:\n",
    "            t = ''\n",
    "            text = c[-1].lower()\n",
    "            if 'support' in text or 'yes' in text or 'yep' in text:\n",
    "                t = 'Support'\n",
    "            elif 'oppose' in text:\n",
    "                t = 'Oppose'\n",
    "            elif 'defer' in text or 'neutral' in text or 'ambivalent' in text:\n",
    "                t = 'Neutral'\n",
    "            # assuming a comment that we cannot parse is not a vote\n",
    "            else:\n",
    "                continue\n",
    "            all_comments.append([t] + c)\n",
    "    if len(all_comments) == 0:\n",
    "        print('Found no comments for link {}'.format(link))\n",
    "    self_nominate = is_self_nominate(content, link, user_name)            \n",
    "    return (self_nominate, all_comments)\n",
    "\n",
    "def process_row(item, parse_re):\n",
    "    \"\"\"\n",
    "    Process a row of data for bureaucrat election pages only, excluding\n",
    "    any rows that result in success from unsucessful election pages to avoid\n",
    "    duplicating with successful election pages. Successful and unsuccessful election pages\n",
    "    supplement their own regexes for parsing the rows.\n",
    "    \"\"\"\n",
    "    text = item.text.strip()\n",
    "    # the links to the original bureaucrats; not elected\n",
    "    if ',' not in text:\n",
    "        return None\n",
    "    name, phrase = text.split(',')\n",
    "    if '(' in name:\n",
    "        name = name[:name.find('(')]\n",
    "    m = re.match(parse_re, phrase.strip())\n",
    "    if m:\n",
    "        parsed_row = [text.strip() for text in m.groups()]\n",
    "        if (parsed_row[1] == 'promoted' or parsed_row[1] == 'successful'):\n",
    "            return None\n",
    "        try:\n",
    "            link = item.find('a', href=True)\n",
    "            followable_link = link['href']\n",
    "            if 'https' not in followable_link:\n",
    "                followable_link = \"https://en.wikipedia.org\" + followable_link\n",
    "            nested_history = requests.get(followable_link)\n",
    "            nested_soup = BeautifulSoup(nested_history.content, 'html.parser')\n",
    "        except Exception as e:\n",
    "            print(\"Link {} errored out\".format(link))\n",
    "            print(e)\n",
    "        self_nominate, comments = extract_comments(nested_soup, followable_link)\n",
    "        return(([name] + parsed_row + [self_nominate], comments))\n",
    "\n",
    "    else:\n",
    "        print('Could not process {}'.format(text))\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False,\n",
       " [['Support',\n",
       "   '',\n",
       "   '',\n",
       "   'Support. Jwrosenzweig would make a valuable addition to the group of admins if he accepts the responsibility. He gives and recieves advice graciously. All in all the model of what a good admin should be. -- Cimon Avaro on a pogostick'],\n",
       "  ['Support',\n",
       "   '11:47, 21 Aug 2003 (UTC)',\n",
       "   'Fantasy',\n",
       "   'Support.  and other of his actions/comments indicate that he would make a great addition to Wikipedia Adminship. And: I could not find nealy any thing he did without Summary. I like that\\xa0;-) --']])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# link = \"https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Jwrosenzweig\"\n",
    "# soup = requests.get(link)\n",
    "# content = BeautifulSoup(soup.content, 'html.parser')\n",
    "# l = extract_comments(content, link)\n",
    "# l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Successful Elections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Raul654\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Viajero\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Marumari\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Smith03\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Bdesham\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Phil_Bordelon\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Poor_Yorick\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Cyp\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Menchi\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Cimon_avaro\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Angela\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Anthere\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Deb\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Quercusrobur\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Mbecker\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Jtdirl\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Cgs\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Kosebamse\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Kils\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Dante_Alighieri\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Jimfbleak\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/The_Anome\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Hephaestos\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Someone_else\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Minesweeper\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Slrubenstein\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/172\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Kingturtle\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Ams80\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Tannin\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Montrealais\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Notheruser\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/JohnOwens\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Cprompt\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Tim_Starling\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/LittleDan\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/MyRedDice\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Cordyph\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Oliver_Pereira\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Sheldon_Rampton\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Sannse\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Fred_Bauder\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Infrogmation\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Zoe\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Eloquence\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Jasonr\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/XJamRastafire\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/DJ_Clayworth\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Francs2000\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Danny\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Kelly_Martin\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Matt_Britt\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Danny\n",
      "Found no comments for link https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Rootology\n"
     ]
    }
   ],
   "source": [
    "curr_year = 2021\n",
    "parsed_rows = []\n",
    "parsed_comments = []\n",
    "for year in range(2003, curr_year+1):\n",
    "    page = requests.get(\"https://en.wikipedia.org/wiki/Wikipedia:Successful_requests_for_adminship/\"+str(year))\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # assumes one table per page\n",
    "    table = soup.find_all('table')[0]\n",
    "    rows = table.find_all('tr')\n",
    "    for row in rows:\n",
    "        cells = row.find_all('td')\n",
    "        if (len(cells) == 4):\n",
    "            yes, no, neutral = None, None, None\n",
    "            if len(cells[3].text) > 0:\n",
    "                cells[3].find('span').decompose()\n",
    "                yes, no, neutral = [int(i) for i in cells[3].text.strip('()').split('/')]\n",
    "            user_name, alt_name = parse_name(cells[0].text)\n",
    "            followable_link = cells[0].find('a')['href']\n",
    "            if 'https' not in followable_link:\n",
    "                followable_link = \"https://en.wikipedia.org\" + followable_link\n",
    "            nested_history = requests.get(followable_link)\n",
    "            nested_soup = BeautifulSoup(nested_history.content, 'html.parser')\n",
    "            self_nominate, all_comments = extract_comments(nested_soup, followable_link)\n",
    "            date = cells[1].text.strip()\n",
    "            parsed_rows.append([user_name, date, alt_name, cells[2].text.strip(), yes, no, neutral, self_nominate])\n",
    "            for c in all_comments:\n",
    "                pc = [user_name, date] + c\n",
    "                parsed_comments.append(pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['user_name', 'date', 'alternative_name', 'closed by', 'yes', 'no', 'neutral', 'self_nominate']\n",
    "successful_elections = pd.DataFrame(parsed_rows, columns=header)\n",
    "successful_elections['date'] = successful_elections['date'].apply(lambda x : parse_date(x))\n",
    "successful_elections = successful_elections.sort_values(by=['date', 'user_name'])\n",
    "successful_elections['num_nomination'] = successful_elections.groupby(['user_name']).cumcount()+1\n",
    "successful_elections.to_csv('~/Documents/Stanford/Research/Network/processed_data/successful_elections_rfa.csv', index=False)\n",
    "\n",
    "comment_header = ['user_name', 'date', 'type', 'comment_date', 'comment_name', 'comment']\n",
    "successful_elections_comments = pd.DataFrame(parsed_comments, columns=comment_header)\n",
    "successful_elections_comments['date'] = successful_elections_comments['date'].apply(lambda x : parse_date(x))\n",
    "successful_elections_comments['comment_date'] = successful_elections_comments['comment_date'].apply(lambda x : parse_time(x))\n",
    "successful_elections_comments = successful_elections_comments.sort_values(by=['date', 'user_name'])\n",
    "successful_elections_comments.to_csv('~/Documents/Stanford/Research/Network/processed_data/successful_elections_rfa_comments.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Unsuccessful Elections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data from 2004\n",
      "Processing data from 2005\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c915699264ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mnested_soup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0myes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneutral\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_votes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_soup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mself_nominate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_comments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_comments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_soup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollowable_link\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m', \\((.+)\\)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"(.+)[ ,]+(\\d+ \\w+,? ?\\d+) *[-, ]+ ?([\\w ]+)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-4f049f327f9f>\u001b[0m in \u001b[0;36mextract_comments\u001b[0;34m(content, link)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mcomments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'toc'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'toc'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'mw-parser-output'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ul'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0;31m# could either be at the comment level or the nomination level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'li'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "curr_year = 2021\n",
    "parsed_rows = []\n",
    "parsed_comments = []\n",
    "# no closed by header as they aren't available for early unsuccessful RfA elections\n",
    "# no unsuccessful election data available for 2003, table format available for 2008 forward\n",
    "for year in range(2004, 2008):\n",
    "    print(\"Processing data from {}\".format(year))\n",
    "    page = requests.get(\"https://en.wikipedia.org/wiki/Wikipedia:Unsuccessful_adminship_candidacies_(Chronological)/\"+str(year))\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    content = soup.find('div', {'mw-parser-output'})\n",
    "    lists = content.find_all('ul', recursive=False)\n",
    "    for l in lists:\n",
    "        items = l.find_all('li')\n",
    "        # each row \n",
    "        for i in items:\n",
    "            try:\n",
    "                link=i.find('a', href=True)\n",
    "                followable_link = link['href']\n",
    "                if 'https' not in followable_link:\n",
    "                    followable_link = \"https://en.wikipedia.org\" + followable_link\n",
    "                nested_history = requests.get(followable_link)\n",
    "            except Exception as e:\n",
    "                print(\"Link {} errored out\".format(link))\n",
    "                print(e)\n",
    "            nested_soup = BeautifulSoup(nested_history.content, 'html.parser')\n",
    "            yes, no, neutral = find_votes(nested_soup)\n",
    "            self_nominate, all_comments = extract_comments(nested_soup, followable_link)\n",
    "            text = re.sub(', \\((.+)\\)', '', i.text)\n",
    "            m = re.match(r\"(.+)[ ,]+(\\d+ \\w+,? ?\\d+) *[-, ]+ ?([\\w ]+)\", text)\n",
    "            if m:\n",
    "                parsed_row = [text.strip().replace(',', '') for text in m.groups()]\n",
    "            else:\n",
    "                print(\"Failed parsing row {}\".format(i.text))\n",
    "                continue\n",
    "            user_name, alternative_name = parse_name(parsed_row[0])\n",
    "            date = parsed_row[1].strip()\n",
    "            # closed by not parsed\n",
    "            parsed_rows.append([user_name, date, alternative_name, parsed_row[2].strip(), yes, no, neutral, self_nominate])\n",
    "            for c in all_comments:\n",
    "                parsed_comments.append([user_name, date] + c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data in tabulated format 2008 onwards \n",
    "for year in range(2008, curr_year+1):\n",
    "    print(\"Processing data from {}\".format(year))\n",
    "    page = requests.get(\"https://en.wikipedia.org/wiki/Wikipedia:Unsuccessful_adminship_candidacies_(Chronological)/\"+str(year))\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # assumes one table per page\n",
    "    table = soup.find_all('table')[0]\n",
    "    rows = table.find_all('tr')\n",
    "    for row in rows:\n",
    "        cells = row.find_all('td')\n",
    "        if (len(cells) == 5):\n",
    "            yes, no, neutral = None, None, None\n",
    "            if len(cells[4].text) > 0:\n",
    "                cells[4].find('span').decompose()\n",
    "                yes, no, neutral = [int(i) for i in cells[4].text.strip('()').split('/')]\n",
    "            # from 2008 forward, parentheses contain the number of relection instead of alternative username\n",
    "            user_name, _ = parse_name(cells[0].text)\n",
    "            date = cells[1].text.strip()\n",
    "            followable_link = cells[0].find('a')['href']\n",
    "            if 'https' not in followable_link:\n",
    "                followable_link = \"https://en.wikipedia.org\" + followable_link\n",
    "            nested_history = requests.get(followable_link)\n",
    "            nested_soup = BeautifulSoup(nested_history.content, 'html.parser')\n",
    "            self_nominate, all_comments = extract_comments(nested_soup, followable_link)\n",
    "            parsed_rows.append((user_name, date, '', cells[2].text.strip(), yes, no, neutral, self_nominate))\n",
    "            for c in all_comments:\n",
    "                parsed_comments.append([user_name, date] + c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['user_name', 'date', 'alternative_name', 'result', 'yes', 'no', 'neutral', 'self_nominate']\n",
    "unsuccessful_elections = pd.DataFrame(parsed_rows, columns=header)\n",
    "unsuccessful_elections['date'] = unsuccessful_elections['date'].apply(lambda x : parse_date(x))\n",
    "unsuccessful_elections = unsuccessful_elections.sort_values(by=['date', 'user_name'])\n",
    "unsuccessful_elections['num_nomination'] = unsuccessful_elections.groupby(['user_name']).cumcount()+1\n",
    "unsuccessful_elections.to_csv('~/Documents/Stanford/Research/Network/processed_data/unsuccessful_elections_rfa.csv', index=False)\n",
    "\n",
    "\n",
    "comment_header = ['user_name', 'date', 'type', 'comment_date', 'comment_name', 'comment']\n",
    "unsuccessful_elections_comments = pd.DataFrame(parsed_comments, columns=comment_header)\n",
    "unsuccessful_elections_comments['date'] = unsuccessful_elections_comments['date'].apply(lambda x : parse_date(x))\n",
    "unsuccessful_elections_comments['comment_date'] = unsuccessful_elections_comments['comment_date'].apply(lambda x : parse_time(x))\n",
    "unsuccessful_elections_comments = unsuccessful_elections_comments.sort_values(by=['date', 'user_name'])\n",
    "unsuccessful_elections_comments.to_csv('~/Documents/Stanford/Research/Network/processed_data/unsuccessful_elections_rfa_comments.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Successful Bureaucrat Elections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"https://en.wikipedia.org/wiki/Wikipedia:Successful_bureaucratship_candidacies\"\n",
    "parsed_rows = []\n",
    "parsed_comments = []\n",
    "success_parse_re = r\"closed (\\d+ \\w+ \\d+) by (.+) at +\\((\\d+)/(\\d+)/(\\d+)\\)\"\n",
    "page = requests.get(link)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "content = soup.find('div', {'mw-parser-output'})\n",
    "content.find('div', {'class':'navbox'}).decompose()\n",
    "content.find('div', {'toc'}).decompose()\n",
    "lists = content.find_all('ul')\n",
    "for l in lists:\n",
    "    items = l.find_all('li')\n",
    "    for i in items:\n",
    "        result = process_row(i, success_parse_re)\n",
    "        if result:\n",
    "            row, comments = result\n",
    "            parsed_rows.append(row)\n",
    "            for c in comments:\n",
    "                parsed_comments.append([row[0], row[1]] + c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['user_name', 'date', 'closed by', 'yes', 'no', 'neutral', 'self_nominate']\n",
    "successful_elections = pd.DataFrame(parsed_rows, columns=header)\n",
    "successful_elections['date'] = successful_elections['date'].apply(lambda x : parse_date(x))\n",
    "successful_elections = successful_elections.sort_values(by=['date', 'user_name'])\n",
    "successful_elections['num_nomination'] = successful_elections.groupby(['user_name']).cumcount()+1\n",
    "successful_elections.to_csv('~/Documents/Stanford/Research/Network/processed_data/successful_elections_rfb.csv', index=False)\n",
    "\n",
    "\n",
    "comment_header = ['user_name', 'date', 'type', 'comment_date', 'comment_name', 'comment']\n",
    "successful_elections_comments = pd.DataFrame(parsed_comments, columns=comment_header)\n",
    "successful_elections_comments['comment_date'] = successful_elections_comments['comment_date'].apply(lambda x : parse_time(x))\n",
    "successful_elections_comments['date'] = successful_elections_comments['date'].apply(lambda x : parse_date(x))\n",
    "successful_elections_comments = successful_elections_comments.sort_values(by=['date', 'user_name'])\n",
    "successful_elections_comments.to_csv('~/Documents/Stanford/Research/Network/processed_data/successful_elections_rfb_comments.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Unsuccessful Buraucrat Elections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"https://en.wikipedia.org/wiki/Wikipedia:Unsuccessful_bureaucratship_candidacies\"\n",
    "parsed_rows = []\n",
    "parsed_comments = []\n",
    "page = requests.get(link)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "content = soup.find('div', {'mw-parser-output'})\n",
    "content.find('div', {'class':'navbox'}).decompose()\n",
    "content.find('div', {'toc'}).decompose()\n",
    "lists = content.find_all('ul')\n",
    "parse_re = r\"(\\d+ \\w+ \\d+) [â€“-] (.+) a?t? ?\\((\\d+)/(\\d+)/(\\d+)\\)\"\n",
    "for l in lists:\n",
    "    items = l.find_all('li', recursive=False)\n",
    "    for i in items:\n",
    "        nested_list = i.find('ul')\n",
    "        if nested_list:\n",
    "            nested_items = nested_list.find_all('li')\n",
    "            for ni in nested_items:\n",
    "                result = process_row(ni, parse_re)\n",
    "                if result:\n",
    "                    row, comments = result\n",
    "                    parsed_rows.append(row)\n",
    "                    for c in comments:\n",
    "                        parsed_comments.append([row[0], row[1]] + c)\n",
    "            i.find('ul').decompose()\n",
    "        result = process_row(i, parse_re)\n",
    "        if result:\n",
    "            row, comments = result\n",
    "            parsed_rows.append(row)\n",
    "            for c in comments:\n",
    "                parsed_comments.append([row[0], row[1]] + c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['user_name', 'date', 'closed by', 'yes', 'no', 'neutral', 'self_nominate']\n",
    "unsuccessful_elections = pd.DataFrame(parsed_rows, columns=header)\n",
    "unsuccessful_elections['date'] = unsuccessful_elections['date'].apply(lambda x : parse_date(x))\n",
    "unsuccessful_elections = unsuccessful_elections.sort_values(by=['date', 'user_name'])\n",
    "unsuccessful_elections['num_nomination'] = unsuccessful_elections.groupby(['user_name']).cumcount()+1\n",
    "unsuccessful_elections.to_csv('~/Documents/Stanford/Research/Network/processed_data/unsuccessful_elections_rfb.csv', index=False)\n",
    "\n",
    "comment_header = ['user_name', 'date', 'type', 'comment_date', 'comment_name', 'comment']\n",
    "unsuccessful_elections_comments = pd.DataFrame(parsed_comments, columns=comment_header)\n",
    "unsuccessful_elections_comments['comment_date'] = unsuccessful_elections_comments['comment_date'].apply(lambda x : parse_time(x))\n",
    "unsuccessful_elections_comments['date'] = unsuccessful_elections_comments['date'].apply(lambda x : parse_date(x))\n",
    "unsuccessful_elections_comments = unsuccessful_elections_comments.sort_values(by=['date', 'user_name'])\n",
    "unsuccessful_elections_comments.to_csv('~/Documents/Stanford/Research/Network/processed_data/unsuccessful_elections_rfb_comments.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
